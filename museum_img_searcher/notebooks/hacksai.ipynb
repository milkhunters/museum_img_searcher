{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8103060,"sourceType":"datasetVersion","datasetId":4785440}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\n\n\nclass MuseumDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Путь к CSV файлу со списком изображений.\n            root_dir (string): Директория с изображениями.\n            transform (callable, optional): Преобразование, применяемое к изображению.\n        \"\"\"\n        self.annotations = pd.read_csv(csv_file, delimiter=';')\n        self.root_dir = root_dir\n        self.transform = transform\n        \n        self.images = ImageFolder(root=root_dir, transform=transform)\n        self.image_paths = {os.path.basename(path): path for path, _ in self.images.imgs}\n\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        image_name = self.annotations.iloc[idx, 4]\n        image = Image.open(self.image_paths.get(image_name)).convert(\"RGB\")\n\n        if self.transform is not None:\n            image = self.transform(image)\n            \n\n        group = self.annotations.iloc[idx, 3]\n        desc = self.annotations.iloc[idx, 2]\n        name = self.annotations.iloc[idx, 1]\n\n        sample = {\n            'image': image,\n            'group': group\n        }\n        \n        return sample\n\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((256, 256), antialias=True),\n    transforms.ConvertImageDtype(torch.float),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ndataset = MuseumDataset(csv_file='/kaggle/input/mincult-museum-dataset/train_dataset_mincult-train/train.csv', root_dir='/kaggle/input/mincult-museum-dataset/train_dataset_mincult-train/train', transform=transform)\n\nindices = list(range(len(dataset)))\ntrain_indices, test_indices = train_test_split(indices, test_size=0.5, random_state=42)\n\ntrain_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\ntest_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=32, sampler=train_sampler)\ntest_loader = torch.utils.data.DataLoader(dataset, batch_size=32, sampler=test_sampler)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T08:43:31.560026Z","iopub.execute_input":"2024-04-13T08:43:31.560822Z","iopub.status.idle":"2024-04-13T08:43:45.099861Z","shell.execute_reply.started":"2024-04-13T08:43:31.560788Z","shell.execute_reply":"2024-04-13T08:43:45.098797Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.functional import cosine_similarity\n\nclass ImageEncoder(nn.Module):\n    def __init__(self):\n        super(ImageEncoder, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.pulling = nn.AdaptiveAvgPool2d(4)\n        \n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.linear = nn.Linear(2048, 128)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pulling(x)\n        x = F.relu(self.conv2(x))\n        x = torch.flatten(x, start_dim=1)\n        #print(x.shape)\n        x = self.linear(x)\n        return x\n\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, target):\n\n        cosine_distance = cosine_similarity(output1, output2)\n\n        loss_contrastive = torch.mean((1 - target) * torch.pow(cosine_distance, 2) +\n                                      (target) * torch.pow(torch.clamp(self.margin - cosine_distance, min=0.0), 2))\n        return loss_contrastive","metadata":{"execution":{"iopub.status.busy":"2024-04-13T08:43:45.101779Z","iopub.execute_input":"2024-04-13T08:43:45.102106Z","iopub.status.idle":"2024-04-13T08:43:45.114541Z","shell.execute_reply.started":"2024-04-13T08:43:45.102078Z","shell.execute_reply":"2024-04-13T08:43:45.113616Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\ndevice = 'cuda'\nencoder = ImageEncoder().to(device)\ncriterion = ContrastiveLoss().to(device)\noptimizer = optim.Adam(encoder.parameters(), lr=0.05)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T08:43:45.115906Z","iopub.execute_input":"2024-04-13T08:43:45.116243Z","iopub.status.idle":"2024-04-13T08:43:45.396498Z","shell.execute_reply.started":"2024-04-13T08:43:45.116205Z","shell.execute_reply":"2024-04-13T08:43:45.395608Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport math\n\nlosses = []\nnum_epochs = 5\n\ncriterion = ContrastiveLoss().to(device)\n\nfor epoch in range(num_epochs):\n    encoder.train()\n    running_loss = 0.0\n    \n    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}', leave=False):\n        inputs = batch['image'].to(device)\n        labels = [item for item in batch['group']]\n\n        batch_size = inputs.size(0)\n        images1, images2 = torch.split(inputs, batch_size // 2, dim=0)\n\n        optimizer.zero_grad()\n\n        outputs1 = encoder(images1)\n        outputs2 = encoder(images2)\n\n        target = [1.0 if labels[0] == labels[1] else 0.0]\n\n        loss = criterion(outputs1, outputs2, torch.tensor(target).to(device)) \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n\n    average_loss = running_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}, Loss: {average_loss}\")\n    losses.append(average_loss)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T08:43:48.106974Z","iopub.execute_input":"2024-04-13T08:43:48.107570Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/509 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30e7d7e0511b48629ea0894378389f08"}},"metadata":{}}]}]}